{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "np.set_printoptions(precision=10)\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(z, 0)\n",
    "\n",
    "def add_bias(x):\n",
    "    return np.append(x, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 3) (3,)\n",
      "w: [[ 1  0 -1]\n",
      " [ 0  1 -1]\n",
      " [-1  0 -1]\n",
      " [ 0 -1 -1]] \n",
      "\n",
      "x: [ 3 14  1]\n",
      "z1: [  2  13  -4 -15] \n",
      "\n",
      "a1: [ 2 13  0  0] \n",
      "\n",
      "(2, 5) (5,)\n",
      "[0.9999996941, 3.059022269e-07]\n"
     ]
    }
   ],
   "source": [
    "x = add_bias(np.array([3, 14]).reshape((2, 1)))\n",
    "\n",
    "w = np.array([[1, 0, -1], [0, 1, -1], [-1, 0, -1], [0, -1, -1]])\n",
    "v = np.array([[1, 1, 1, 1, 0], [-1, -1, -1, -1, 2]])\n",
    "\n",
    "print(w.shape, x.shape)\n",
    "print('w:', w, '\\n')\n",
    "print('x:', x, )\n",
    "\n",
    "z1 = w @ x\n",
    "print('z1:', z1, '\\n')\n",
    "\n",
    "a1 = relu(w @ x)\n",
    "print('a1:', a1, '\\n')\n",
    "\n",
    "a1 = add_bias(a1)\n",
    "\n",
    "print(v.shape, a1.shape)\n",
    "z2 = relu(v @ a1)\n",
    "a2 = softmax(z2)\n",
    "\n",
    "print(f'[{a2[0]:.10}, {a2[1]:.10}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z2: [[1.]\n",
      " [1.]] \n",
      "\n",
      "a2: [[0.5]\n",
      " [0.5]] \n",
      "\n",
      "z2: [[0]\n",
      " [2]] \n",
      "\n",
      "a2: [[0.119202922]\n",
      " [0.880797078]] \n",
      "\n",
      "z2: [[3.]\n",
      " [0.]] \n",
      "\n",
      "a2: [[0.9525741268]\n",
      " [0.0474258732]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "v = np.array([[1, 1, 1, 1, 0], [-1, -1, -1, -1, 2]])\n",
    "\n",
    "def calc_o1(a1):\n",
    "    a1 = add_bias(np.array(a1)).reshape(5, 1)\n",
    "    z2 = relu(v @ a1)\n",
    "    print('z2:', z2, '\\n')\n",
    "    a2 = softmax(z2)\n",
    "    print('a2:', a2, '\\n')\n",
    "\n",
    "calc_o1([0.25, 0.25, 0.25, 0.25])\n",
    "calc_o1([0, 0, 0, 0])\n",
    "calc_o1([0.75, 0.75, 0.75, 0.75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148.4131591025766\n"
     ]
    }
   ],
   "source": [
    "def softmax_with_temp(x, t):\n",
    "    return np.exp(t*x) / np.sum(np.exp(t*x))\n",
    "\n",
    "softmax_with_temp(np.array([3, 1]), 1)\n",
    "\n",
    "print(np.exp(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [0, 0, 1, 1, 1, 0]\n",
      "h: [0, 0, 1, -1, 1, 0]\n",
      "\n",
      "x: [1, 1, 0, 1, 1]\n",
      "h: [1, -1, 0, 1, -1]\n",
      "\n",
      "x: [1, 0, 1, 1, 1]\n",
      "h: [1, 0, 1, -1, 1]\n",
      "\n",
      "x: [1, 1, 0, 1, 1, 0, 1, 1, 1]\n",
      "h: [1, -1, 0, 1, -1, 0, 1, -1, 1]\n",
      "\n",
      "x: [1, 0, 0, 1, 1, 0, 1, 1, 1]\n",
      "h: [1, 0, 0, 1, -1, 0, 1, -1, 1]\n",
      "\n",
      "x: [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
      "h: [0, 0, 0, 0, 0, 1, -1, 1, -1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Question 2 - LSTM\n",
    "\n",
    "def sigmoid(x):\n",
    "    return round(1 / (1 + np.exp(-x)))\n",
    "\n",
    "def forget_gate(h, x):\n",
    "    return sigmoid(w_fh * h + w_fx * x + b_f)\n",
    "\n",
    "def input_gate(h, x):\n",
    "    return sigmoid(w_ih * h + w_ix * x + b_i)\n",
    "\n",
    "def output_gate(h, x):\n",
    "    return sigmoid(w_oh * h + w_ox * x + b_o)\n",
    "\n",
    "def memory_cell(c_prev, h, x):\n",
    "    f = forget_gate(h, x)\n",
    "    i = input_gate(h, x)\n",
    "    return f * c_prev + i * round(np.tanh(w_ch * h + w_cx * x + b_c))\n",
    "\n",
    "def hidden_state(c_prev, h, x):\n",
    "    o = output_gate(h, x)\n",
    "    c_new = memory_cell(c_prev, h, x)\n",
    "    h_new = o * round(np.tanh(c_new))\n",
    "    return c_new, h_new\n",
    "\n",
    "# Memory cell weights\n",
    "w_fh = 0\n",
    "w_ih = 0\n",
    "w_oh = 0\n",
    "w_ch = -100\n",
    "\n",
    "# Input weights\n",
    "w_fx = 0\n",
    "w_ix = 100\n",
    "w_ox = 100\n",
    "w_cx = 50\n",
    "\n",
    "# Biases\n",
    "b_f = -100\n",
    "b_i = 100\n",
    "b_o = 0\n",
    "b_c = 0\n",
    "\n",
    "def rnn(seq):\n",
    "    c = 0\n",
    "    h = 0\n",
    "    states = []\n",
    "    for x in seq:\n",
    "        c, h = hidden_state(c, h, x)\n",
    "        states += [h]\n",
    "        #print(f'x: {x}, c: {c}, h: {h}')\n",
    "    print(f'x: {seq}\\nh: {[int(i) for i in states]}')\n",
    "        \n",
    "        \n",
    "rnn([0, 0, 1, 1, 1, 0])\n",
    "print('')\n",
    "rnn([1, 1, 0, 1, 1])\n",
    "print('')\n",
    "rnn([1, 0, 1, 1, 1])\n",
    "print('')\n",
    "rnn([1, 1, 0, 1, 1, 0, 1, 1, 1])\n",
    "print('')\n",
    "rnn([1, 0, 0, 1, 1, 0, 1, 1, 1])\n",
    "print('')\n",
    "rnn([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.288\n",
      "vars [0.03, 0.03, -1.15, 0.24048908305088898]\n"
     ]
    }
   ],
   "source": [
    "# Forward Propagation\n",
    "\n",
    "y = 1\n",
    "x = 3\n",
    "w_1 = 0.01\n",
    "w_2 = -5\n",
    "b = -1\n",
    "\n",
    "z_1 = w_1 * x\n",
    "a_1 = np.maximum(0, z_1)\n",
    "z_2 = w_2 * a_1 + b\n",
    "a_2 = 1 / (1 + np.exp(-z_2))\n",
    "loss = 1/2 * (y - a_2)**2\n",
    "print('loss', round(loss, 3))\n",
    "print('vars', [z_1, a_1, z_2, a_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss wrt w2: -0.004\n",
      "Loss wrt b: -0.139\n",
      "Loss wrt w1: 2.081\n"
     ]
    }
   ],
   "source": [
    "# Backward Propagation\n",
    "\n",
    "dloss = -(y-a_2)\n",
    "da2_z2 = a_2 * (1 - a_2)\n",
    "dloss_z2 = dloss * da2_z2 # chain rule\n",
    "\n",
    "dz2_a1 = w_2\n",
    "dz_w2 = a_1\n",
    "dloss_a1 = dloss_z2 * dz2_a1\n",
    "dloss_w2 = dloss_z2 * dz_w2\n",
    "\n",
    "da1_z1 = 1 # relu derivative 1 > 0\n",
    "dloss_z1 = dloss_a1 * da1_z1\n",
    "\n",
    "dz1_w1 = x\n",
    "dloss_w1 = dloss_z1 * dz1_w1\n",
    "\n",
    "print('Loss wrt w2:', round(dloss_w2, 3))\n",
    "print('Loss wrt b:', round(dloss_z2, 3))\n",
    "print('Loss wrt w1:', round(dloss_w1, 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
